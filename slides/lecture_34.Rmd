---
author: "Dr. Ciaran Evans"
title: Inference with logistic regression models
output:
  xaringan::moon_reader:
    css: "lab-slides.css"
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

## Agenda

---

### Recap: maximum likelihood estimation for logistic regression

---

## Deviance

**Deviance:** If $L$ is the likleihood, then deviance is given by $-2 \log L$

* Maximizing likelihood is equivalent to minimizing deviance
* Deviance is analogous to SSE from linear regression
    * In fact, if $y = \beta_0 + \beta_1 x + \varepsilon$ and $\varepsilon \sim N(0, \sigma_\varepsilon^2)$, then deviance = SSE
    
---

## Example: GPA and med school admission

$\log \left( \dfrac{\pi}{1 - \pi} \right) = \beta_0 + \beta_1 \text{GPA}$

```{r, include=F}
library(tidyverse)
library(Stat2Data)
library(knitr)
data("MedGPA")

hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
   lines <- options$output.lines
   if (is.null(lines)) {
     return(hook_output(x, options))  # pass to default hook
   }
   x <- unlist(strsplit(x, "\n"))
   more <- "..."
   if (length(lines)==1) {        # first n lines
     if (length(x) > lines) {
       # truncate the output, but add ....
       x <- c(head(x, lines), more)
     }
   } else {
     x <- c(more, x[lines], more)
   }
   # paste these lines together
   x <- paste(c(x, ""), collapse = "\n")
   hook_output(x, options)
 })
```

```{r, output.lines=10:19, highlight.output=c(11)}
med_glm <- glm(Acceptance ~ GPA, data = MedGPA, family=binomial)
summary(med_glm)
```

---

## Comparing deviances

```{r, echo=F, output.lines=10:19, highlight.output=c(10,11)}
med_glm <- glm(Acceptance ~ GPA, data = MedGPA, family=binomial)
summary(med_glm)
```

75.791 = deviance for intercept-only model $\hspace{0.5cm} \log \left( \dfrac{\pi}{1 - \pi} \right) = \beta_0$ 

56.839 = deviance for full model $\hspace{0.5cm} \log \left( \dfrac{\pi}{1 - \pi} \right) = \beta_0 + \beta_1 \text{GPA}$

--

**drop-in-deviance:** deviance for reduced model - deviance for full model = 18.952

---

## Comparing deviances

75.791 = deviance for intercept-only model $\hspace{0.5cm} \log \left( \dfrac{\pi}{1 - \pi} \right) = \beta_0$ 

56.839 = deviance for full model $\hspace{0.5cm} \log \left( \dfrac{\pi}{1 - \pi} \right) = \beta_0 + \beta_1 \text{GPA}$

**drop-in-deviance:** deviance for reduced model - deviance for full model = 18.952

.question[
Intuition: a larger drop in deviance is stronger evidence for a relationship between GPA and med school acceptance
]

---

## Comparing deviances

**drop-in-deviance:** $G =$ deviance for reduced model - deviance for full model = 18.952

Full model: $\hspace{0.5cm} \log \left( \dfrac{\pi}{1 - \pi} \right) = \beta_0 + \beta_1 \text{GPA}$ 

* degrees of freedom: $df_{\text{full}} = n - p = 55 - 2 = 53$

Reduced model: $\hspace{0.5cm} \log \left( \dfrac{\pi}{1 - \pi} \right) = \beta_0$

* degrees of freedom: $df_{\text{reduced}} = n - p = 55 - 1 = 54$

--

$H_0: \beta_1 = 0 \hspace{1cm} H_A: \beta_1 \neq 0$

Under $H_0$, $G \sim \chi^2_{df_{\text{reduced}} - df_{\text{full}}}$

---

## $\chi^2$ distribution

$\chi^2_k$ distribution: parameterized by degrees of freedom $k$

.center[
<img src="Chi-square_pdf.png" width="600">
]


---

## $\chi^2$ distribution

$\chi^2_k$ distribution: parameterized by degrees of freedom $k$

.center[
<img src="Chi-square_pdf.png" width="400">
]

$G =$ deviance for reduced model - deviance for full model

Under $H_0$, $G \sim \chi^2_{df_{\text{reduced}} - df_{\text{full}}}$

.question[
Why is $G$ always $\geq 0$?
]

---

## Computing a p-value

$\log \left( \dfrac{\pi}{1 - \pi} \right) = \beta_0 + \beta_1 \text{GPA}$ 

$H_0: \beta_1 = 0 \hspace{1cm} H_A: \beta_1 \neq 0$

$G =$ deviance for reduced model - deviance for full model = 18.952 $\sim \chi^2_1$

```{r}
pchisq(18.952, df = 1, lower.tail=FALSE)
```

---

## Likelihood ratio test for nested models

* Compare full and reduced models. Example:
    * full model: $\log \left( \dfrac{\pi}{1 - \pi} \right) = \beta_0 + \beta_1 \text{GPA}$
    * reduced model: $\log \left( \dfrac{\pi}{1 - \pi} \right) = \beta_0$

---

## Likelihood ratio test for nested models

* Compare full and reduced models
* Calculate deviance ( $-2 \log L$ ) for full and reduced models. Example:
    * deviance for full model = 56.839
    * deviance for reduced model = 75.791
---

## Likelihood ratio test for nested models

* Compare full and reduced models
* Calculate deviance ( $-2 \log L$ ) for full and reduced models
* Test statistic: $G =$ deviance for reduced model - deviance for full model
    * Example: $G = 75.791 - 56.839 = 18.952$
--
* p-value: $G \sim \chi^2_{df_{\text{reduced}} - df_{\text{full}}}$
    * Example: $G \sim \chi^2_1$

---

## Alternative: Wald tests for single parameters

$\log \left( \dfrac{\pi}{1 - \pi} \right) = \beta_0 + \beta_1 \text{GPA}$ 

$H_0: \beta_1 = 0 \hspace{1cm} H_A: \beta_1 \neq 0$

$z = \dfrac{\widehat{\beta}_1 - 0}{SE_{\widehat{\beta}_1}} = \dfrac{\widehat{\beta}_1}{SE_{\widehat{\beta}_1}} \hspace{0.5cm} \sim N(0, 1)$

---

## Example

$\log \left( \dfrac{\pi}{1 - \pi} \right) = \beta_0 + \beta_1 \text{GPA}$ 

$H_0: \beta_1 = 0 \hspace{1cm} H_A: \beta_1 \neq 0$

```{r, echo=F, output.lines=10:14, highlight.output=c(4)}
summary(med_glm)
```

$z = \dfrac{5.454}{1.579} = 3.454 \hspace{0.5cm} \sim N(0, 1)$

p-value = 0.000553

---

## Wald tests vs. likelihood ratio tests

.pull-left[
**Wald test**

* like t-tests
* test a single parameter
* some example hypotheses:
    * $H_0: \beta_1 = 0$ vs. $H_A: \beta_1 \neq 0$
    * $H_0: \beta_1 = 0$ vs. $H_A: \beta_1 > 0$
        * Note: for one-sided tests, use the `pnorm` function in R, rather than the summary output
]

.pull-right[
**Likelihood ratio test**

* like nested F-tests
* test one or more parameters 
* some example hypotheses:
    * $H_0: \beta_1 = 0$ vs. $H_A: \beta_1 \neq 0$
    * In multiple logistic regression, could have 
    $H_0: \beta_1 = \beta_2 = \beta_3 = 0$
    $H_A: \text{ at least one of } \beta_1, \beta_2, \beta_3 \neq 0$
]

p-values are different, because test statistics and distributions are different

---

## Confidence intervals

Confidence interval for $\beta_1$:

.center[
$\widehat{\beta}_1 \pm z^* SE_{\widehat{\beta}_1}$
]

where $z^* =$ critical value of $N(0, 1)$ distribution.

---

## Computing $z^*$

Example: for a 95% confidence interval, $z^* = 1.96$

```{r}
qnorm(0.025, lower.tail=F)
```

Example: for a 99% confidence interval, $z^* = 2.58$

```{r}
qnorm(0.005, lower.tail=F)
```

---

## Example

$\log \left( \dfrac{\pi}{1 - \pi} \right) = \beta_0 + \beta_1 \text{GPA}$ 

```{r, echo=F, output.lines=10:14, highlight.output=c(4)}
summary(med_glm)
```

95% confidence interval for $\beta_1$: 

.center[
$5.454 \pm 1.96 \cdot 1.579 = (2.36, 8.55)$
]